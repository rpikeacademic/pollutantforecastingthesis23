{"cells":[{"cell_type":"markdown","metadata":{},"source":["### This file is for AFTER Google Colab updated on 2023-07-21\n","Important things to note:\n","1. Colab's Ubuntu upgraded from 20.04 LTS to 22.04 LTS\n","2. Colab's CUDA upgraded, and downgrading it was impossible due to Ubuntu's upgrade.\n","\n","These changes required use of the latest MXNet package made for cuda 11.2, `mxnet-cu112`,\n","which worked on the newer version. Also, MXNet became officially archived."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1701571313975,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"4eTd8BEgsq7t","outputId":"47bd83f1-658a-4c39-f037-e7595bc6359d"},"outputs":[{"name":"stdout","output_type":"stream","text":["DISTRIB_ID=Ubuntu\n","DISTRIB_RELEASE=22.04\n","DISTRIB_CODENAME=jammy\n","DISTRIB_DESCRIPTION=\"Ubuntu 22.04.3 LTS\"\n","PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n","NAME=\"Ubuntu\"\n","VERSION_ID=\"22.04\"\n","VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n","VERSION_CODENAME=jammy\n","ID=ubuntu\n","ID_LIKE=debian\n","HOME_URL=\"https://www.ubuntu.com/\"\n","SUPPORT_URL=\"https://help.ubuntu.com/\"\n","BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n","PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n","UBUNTU_CODENAME=jammy\n"]}],"source":["!cat /etc/*release"]},{"cell_type":"markdown","metadata":{"id":"D0y0SE-M7qA7"},"source":["Validating similar to https://mxnet.apache.org/versions/1.9.1/get_started/validate_mxnet.html"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27415,"status":"ok","timestamp":1701571341621,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"7K1Jyzxmj3cE","outputId":"a7d68966-bc03-405c-9851-7a10da6d4cdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mxnet-cu112\n","  Downloading mxnet_cu112-1.9.1-py3-none-manylinux2014_x86_64.whl (499.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112) (1.23.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-cu112) (2.31.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu112)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-cu112) (2023.11.17)\n","Installing collected packages: graphviz, mxnet-cu112\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","Successfully installed graphviz-0.8.4 mxnet-cu112-1.9.1\n"]}],"source":["!pip install mxnet-cu112"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14304,"status":"ok","timestamp":1701571355922,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"jNYKMLKaj2oe","outputId":"48b9588b-352e-4cfd-8347-dd0c5fa27bc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["number of gpus, expecting 1: 1\n","[[3. 3. 3.]\n"," [3. 3. 3.]]\n"]}],"source":["import mxnet as mx\n","print('number of gpus, expecting 1:',  mx.context.num_gpus())  # 1 gpu\n","a = mx.nd.ones((2, 3), mx.gpu())\n","b = a * 2 + 1\n","print(b.asnumpy())"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1701571355922,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"4zLJApi1TkAM","outputId":"b0792abf-b508-4633-f122-05033bcdf1e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[[3. 3. 3.]\n"," [3. 3. 3.]]\n","<NDArray 2x3 @gpu(0)>\n","numpy dtype: float32\n"]}],"source":["print(b)  # should show it's on the gpu with @gpu(0)\n","print('numpy dtype:', b.asnumpy().dtype)  # should be float32"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1701571355923,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"cDuoiiUWTmLC"},"outputs":[],"source":["del a, b"]},{"cell_type":"markdown","metadata":{"id":"tbyLLGh1m7wn"},"source":["# Importing other packages, and set context"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1701571355923,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"1Il3m2_qcIzC"},"outputs":[],"source":["import mxnet as mx\n","import numpy as np\n","from mxnet import npx\n","from mxnet.gluon import nn, rnn\n","npx.set_np()\n","from mxnet import gluon\n","from mxnet import autograd\n","\n","import time\n","import math"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":119,"status":"ok","timestamp":1701571356031,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"dckvZlYocJ0p"},"outputs":[],"source":["# specify a context\n","ctx = mx.cpu()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701571356031,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"YnzSraDuiQGQ"},"outputs":[],"source":["# WE CAN DO GPU NOW!\n","ctx = mx.gpu()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701571356031,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"9AVEO738bCLo"},"outputs":[],"source":["import pandas as pd\n","\n","import json"]},{"cell_type":"markdown","metadata":{"id":"Ayd90p-CUFUQ"},"source":["# Setting some data processing & NN properties variables used later"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701571356031,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"pAYkh904SR-m","outputId":"93812090-6950-46b7-efdf-1c0bc9fdbe00"},"outputs":[{"name":"stdout","output_type":"stream","text":["sequence length: 7\n","number of neighbors: 3\n","y_feature: PM25\n","x_features: ['PM10', 'CO', 'SO2', 'NO2', 'O3_8', 'WS.max', 'WD.max']\n","number of total cities: 28\n","train cities (21):  [140400, 130200, 130600, 410800, 130400, 140100, 410600, 140300, 371700, 410700, 370300, 371500, 140500, 410200, 130100, 131100, 410500, 371400, 371600, 410900, 110000]\n","test cities (7):  [131000, 410100, 370100, 120000, 130500, 370800, 130900]\n"]}],"source":["# Some variables for data stuff later on\n","SEQ_LENGTH = 7\n","NUM_NEIGHBORS = 3\n","\n","TRAINING_CITIES_RANGE = 'TODO'\n","TESTING_CITIES_RANGE = 'TODO'\n","\n","_DRIVE_ROOT_FLDR = 'drive/MyDrive/Colab Notebooks Project/'\n","\n","STD_COLS = ['PM25', 'PM10', 'CO', 'NO2', 'O3_8', 'SO2']\n","y_feature = 'PM25'\n","x_features = list(set(STD_COLS) - {y_feature}) \\\n","              + ['WS.max', 'WD.max']\n","print('sequence length:', SEQ_LENGTH)\n","print('number of neighbors:', NUM_NEIGHBORS)\n","print('y_feature:', y_feature)\n","print('x_features:', x_features)\n","\n","TRAIN_CITIES_PERCENT = 0.25  # first 25% of the list (below) will be the training cities\n","\n","# order randomized\n","citycodes_list = [\n","    131000,\n","    410100,\n","    370100,\n","    120000,\n","    130500,\n","    370800,\n","    130900,\n","    140400,\n","    130200,\n","    130600,\n","    410800,\n","    130400,\n","    140100,\n","    410600,\n","    140300,\n","    371700,\n","    410700,\n","    370300,\n","    371500,\n","    140500,\n","    410200,\n","    130100,\n","    131100,\n","    410500,\n","    371400,\n","    371600,\n","    410900,\n","    110000,\n","]\n","_cutoff = int( len(citycodes_list) * TRAIN_CITIES_PERCENT )\n","test_citycode_list = citycodes_list[:_cutoff]\n","train_citycode_list = citycodes_list[_cutoff:]\n","del _cutoff\n","print('number of total cities:', len(citycodes_list))\n","print(f'train cities ({len(train_citycode_list)}): ', train_citycode_list)\n","print(f'test cities ({len(test_citycode_list)}): ', test_citycode_list)"]},{"cell_type":"markdown","metadata":{"id":"yboa3o1WzifK"},"source":["# Google drive importing\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1701571356265,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"dQkzufIc0cjz","outputId":"05702a28-82b5-4cfc-98d9-13f2ef583c42"},"outputs":[{"name":"stdout","output_type":"stream","text":["sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1037339,"status":"ok","timestamp":1701572393602,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"zvmi4ZQj0hj1","outputId":"38105b04-fa96-4414-d01f-7c47b45ba569"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2122,"status":"ok","timestamp":1701572396463,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"-qLna2U59e46"},"outputs":[],"source":["!cp 'drive/MyDrive/Colab Notebooks Project/jingjinji.csv' .\n","!cp 'drive/MyDrive/Colab Notebooks Project/aq_.csv' .\n","\n","# params2 contains the saved model parameters and potentially other model-dependent information\n","!cp -r 'drive/MyDrive/Colab Notebooks Project/params2' ./params2\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1701572396752,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"rgEI2T3XaY0m","outputId":"9313fc12-f2d0-4a15-ea64-d86db389f411"},"outputs":[{"name":"stdout","output_type":"stream","text":["hello world\n"]}],"source":["# testing saving to file\n","with open('test_out.txt', 'w') as f:\n","    f.write('hello world')\n","!cp 'test_out.txt' 'drive/MyDrive/Colab Notebooks Project/test_out.txt'\n","\n","# test read from that saved file\n","!cp 'drive/MyDrive/Colab Notebooks Project/test_out.txt' .\n","with open('test_out.txt', 'r') as f:\n","    line_read = f.read()\n","    print(line_read)\n","    assert line_read.rstrip() == 'hello world'"]},{"cell_type":"markdown","metadata":{"id":"BckEutwRUuxl"},"source":["# Copy prepared_seqs from Google Drive"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3800,"status":"ok","timestamp":1701572400551,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"itUFNtVPUuJ7"},"outputs":[],"source":["!cp -r 'drive/MyDrive/Colab Notebooks Project/prepared_seqs' .\n","# where traintype is like:\n","#    > 'together_train_y'\n","#    > 'together_train_allx'\n","#    > 'separate_train_y'\n","#    > 'separate_train_targetx'\n","#    > 'separate_train_neighborx'\n","#  and replace 'train' with 'test' for the other case\n","filename = 'prepared_seqs/{seqtype}_seqs_{num_neighbors}nbrs_seqlen{seq_length}.npy'"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1701572400551,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"GnXxWYtrY84p","outputId":"fbc62554-9d6e-4d62-e60f-571fc2c43ea0"},"outputs":[{"name":"stdout","output_type":"stream","text":[" <class 'dict'>\n"," <class 'float'>\n"," dtype: float64\n"," dtype: float64\n"," dtype: float64\n"," dtype: float64\n"," <class 'dict'>\n"," <class 'float'>\n"," dtype: float64\n"," dtype: float64\n"," dtype: float64\n"," dtype: float64\n"," dtype: float64\n"," dtype: float64\n"]}],"source":["def _npload(seqtype):\n","    a = None\n","    with open(filename.format(seqtype=seqtype, num_neighbors=NUM_NEIGHBORS,\n","                              seq_length=SEQ_LENGTH), 'rb') as f:\n","        a = np.load(f)\n","        print(' dtype:', a.dtype)\n","    return a\n","\n","import pickle\n","def _pload(name):\n","    a = None\n","    with open(f'prepared_seqs/{name}.pickle', 'rb') as f:\n","        a = pickle.load(f)\n","        print('',type(a))\n","    return a\n","\n","together_cities_neighbors_dict = _pload('together_cities_neighbors_dict')\n","together_largestneighbordist = _pload('together_largestneighbordist')\n","\n","together_train_y_seqs = _npload('together_train_y')\n","together_train_allx_seqs = _npload('together_train_allx')\n","together_test_y_seqs = _npload('together_test_y')\n","together_test_allx_seqs = _npload('together_test_allx')\n","\n","separate_cities_neighbors_dict = _pload('separated_cities_neighbors_dict')\n","separate_largestneighbordist = _pload('separated_largestneighbordist')\n","\n","separate_train_y_seqs = _npload('separated_train_y')\n","separate_train_targetx_seqs = _npload('separated_train_targetx')\n","separate_train_neighborx_seqs = _npload('separated_train_neighborx')\n","separate_test_y_seqs = _npload('separated_test_y')\n","separate_test_targetx_seqs = _npload('separated_test_targetx')\n","separate_test_neighborx_seqs = _npload('separated_test_neighborx')"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116,"status":"ok","timestamp":1701573361949,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"DBD17zkGNMrL","outputId":"4115acf7-78b3-4b6e-dcb8-46947692db14"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[-0.7808177]]\n","[[ 2.12809977  0.69022504 -0.49137577  2.73496269  2.50106225  0.24137931\n","   2.81410282  3.05930314  1.93375321 -1.13657368  6.24819473  8.16901703\n","   0.21674877 -0.2270227   0.42444805  1.85701359  2.04495661  1.60935456\n","  -1.18888703  2.73496269  1.64228123  0.4137931   0.31883011  0.56748829\n","   1.34345352  1.16363914  1.17682302 -1.48532931  2.91986964  2.07167174\n","   0.33990148 -0.02907952  0.59649266]\n"," [-0.6987676  -0.98583466 -0.85756918 -0.40845544 -0.16115893  0.21674877\n","   0.64314432  0.38209345 -0.28297091 -0.99707143  0.8858932   0.95525641\n","   0.14778325 -0.24924492  0.42444805 -0.26725762 -0.46596676 -0.28297091\n","  -1.13657368  0.33117235  0.09647538  0.37438424  0.39660789  0.56748829\n","   0.12958425 -0.15002276  0.25769351 -1.01450921  1.07080015  1.04113451\n","   0.22660099 -0.02907952  0.59649266]\n"," [-0.24979455 -0.82363533 -0.77038027 -0.03864155  0.09647538  0.1773399\n","   0.29298973 -0.29968044 -0.76956889 -1.04938477  0.33117235 -0.07528083\n","   0.14778325 -0.26035603  0.42444805 -0.15053943 -0.51585265 -0.71550245\n","  -1.01450921  0.1462654  -0.50467134  0.29064039  0.307719    0.56748829\n","  -0.05716487 -0.53248128 -0.12077159 -1.15401146  0.5160793   0.43998779\n","   0.19704433  0.98463508  0.59649266]\n"," [ 1.52946903  0.90649081 -1.10169812  2.73496269  3.70335569  0.1773399\n","   1.50685899  0.79780924 -0.33703735 -0.85756918  2.55005574  1.04113451\n","   0.16748768 -0.28257825  0.42444805  1.2734226   0.63152293 -0.28297091\n","  -0.77038027  1.44061405 -0.16115893  0.17241379  0.01450323  0.56748829\n","   0.52642613 -0.15002276 -0.06670514 -0.90988252  1.2557071   1.47052502\n","   0.16748768  0.04019063  0.59649266]\n"," [ 0.73129471 -0.06670514 -0.75294249  1.625521    2.07167174  0.22167488\n","   0.87658072  0.43197935 -0.55330312 -0.68319136  1.625521    0.95525641\n","   0.12315271 -0.37146714  0.42444805  1.41348443  0.69803745  0.04142774\n","  -0.82269362  1.44061405 -0.33291513  0.16748768  0.21883011  0.56748829\n","   1.32010988  0.33220756  0.25769351 -0.73550471  0.8858932   0.52586589\n","   0.12315271  0.02907952  0.59649266]\n"," [ 0.29895029 -0.33703735 -0.94475809  1.44061405  2.58694036  0.27093596\n","   0.99329892  0.14929261 -0.44517024 -0.75294249  0.8858932   1.47052502\n","   0.30049261 -0.02853286  0.42444805  2.20716818  1.29666819  0.14956062\n","  -1.03194699  2.18024184  0.43998779  0.18226601 -0.47438566  0.56748829\n","   1.95038815  0.64815156  0.25769351 -1.15401146  1.81042795  1.47052502\n","   0.18226601 -0.54869825  0.59649266]\n"," [-0.78191076 -1.20210043 -0.49137577 -0.59336239  1.12701261  0.63546798\n","  -0.94422318 -0.86505392 -1.58056552 -0.24724683 -1.14808324 -0.16115893\n","   0.46305419  0.1270227   0.42444805 -0.71078678 -0.81516802 -1.41836619\n","  -0.45650021 -0.40845544  0.01059728  0.30049261  0.04783656  0.56748829\n","  -0.4306631  -0.33293771 -0.60736956 -0.66575358  0.1462654   0.7835002\n","   0.45320197  0.6931427   0.59649266]]\n"]}],"source":["print(together_train_y_seqs[1])\n","print(together_train_allx_seqs[1])"]},{"cell_type":"markdown","metadata":{"id":"GJcVhnU_XB6q"},"source":["# Plot test versus train cities on shapefile map"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1701572400706,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"u_02SR85F22X"},"outputs":[],"source":["!cp 'drive/MyDrive/Colab Notebooks Project/joined_cc_names_dict.pickle' .\n","!cp 'drive/MyDrive/Colab Notebooks Project/joined_cc_latlongs_dict.pickle' .\n","import pickle\n","with open('joined_cc_names_dict.pickle', 'rb') as f:\n","  cities_names_dict = pickle.load(f)\n","with open('joined_cc_latlongs_dict.pickle', 'rb') as f:\n","  cities_latlongs_dict = pickle.load(f)\n","print(cities_names_dict)\n","print(cities_latlongs_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1701572400707,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"3kugxW2WlX_u"},"outputs":[],"source":["!cp -r 'drive/MyDrive/Colab Notebooks Project/stanford-bw669kf8724-shapefile/' ."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1701572400707,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"yPvpKWmFW-Ol"},"outputs":[],"source":["# import pandas as pd\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","# import numpy as np\n","\n","\n","N = 256\n","vals = np.ones((N, 4))\n","vals[:, 0] = np.linspace(1, 232/256, N)\n","vals[:, 1] = np.linspace(1, 252/256, N)\n","vals[:, 2] = np.linspace(1, 255/256, N)\n","newcmp = mcolors.ListedColormap(vals)\n","# [0,1] -> [white, very light blue]   used for coloring the jingjinji region(s)\n","\n","\n","# https://towardsdatascience.com/mapping-with-matplotlib-pandas-geopandas-and-basemap-in-python-d11b57ab5dac\n","\n","fp = 'stanford-bw669kf8724-shapefile/bw669kf8724.shp'\n","\n","map_df: gpd.GeoDataFrame = gpd.read_file(fp)\n","\n","\n","# set the Jing-Jin-Ji region provinces to be colored in blue, using the 'jingjinji_mask' column for the ListedColormap. 0 will give it white and 1 will give it the bluish color, set above.\n","\n","map_df['jingjinji_mask'] = 0\n","jingjinji_mask = [i in (2.0, 10.0, 27.0) for i in map_df.id_1.values]  # Beijing Hebei Tianjin\n","map_df.loc[jingjinji_mask, 'jingjinji_mask'] = 1\n","\n","\n","mask = [(lambda i: i in [2.0, 10.0, 27.0  # Beijing Hebei Tianjin\n","                         , 23.0, 12.0, 1.0, 15.0, 31.0, 22.0, 25.0])(i) for i in map_df.id_1.values]  # Shandong Henan Anhui Jiangsu Zhejiang Shaanxi Shanxi\n","print(map_df[mask])\n","map_df = map_df[mask]\n","\n","\n","# plt.rcParams[\"figure.figsize\"] = [16,9]\n","plt.rcParams[\"figure.figsize\"] = [16 * 2/3, 9 * 2/3]  # slightly smaller but same aspect ratio, for text sizing\n","\n","# preview\n","# dfplt = map_df.plot(edgecolor=u'gray', linewidth=0.5, ax=axes1, column='jingjinji_mask', cmap='Blues')\n","# dfplt = map_df.plot(edgecolor=u'gray', linewidth=0.5, ax=axes1, column='jingjinji_mask', cmap=newcmp)\n","dfplt = map_df.plot(edgecolor=u'gray', linewidth=0.5, column='jingjinji_mask', cmap=newcmp)\n","\n","\n","df3ll = {\n","    cc: cities_latlongs_dict[cc]\n","    for cc in set(train_citycode_list)\n","}\n","print(df3ll)\n","df4ll = {\n","    cc: cities_latlongs_dict[cc]\n","    for cc in set(test_citycode_list)\n","}\n","print(df4ll)\n","\n","\n","_swp_args = lambda a1, a2: (a2, a1)\n","dfplt.scatter(*_swp_args(*zip(*df3ll.values())),\n","              c='tab:green',\n","              s=15, label='train cities')\n","dfplt.scatter(*_swp_args(*zip(*df4ll.values())), marker='*',\n","              c='red',\n","              s=15, label='test cities')\n","\n","\n","dfplt.legend(loc = 'upper left')\n","\n","# plt.show()\n","# no need for show, it shows by itself in the notebook."]},{"cell_type":"markdown","metadata":{"id":"HfI6L1wTtYnp"},"source":["# Simple (allx, y) CityDataSet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":150,"status":"aborted","timestamp":1701572400848,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Klf0LN0Wd0xb"},"outputs":[],"source":["class SimpleCityDataSet:\n","\n","    # def __init__(self, city_seqs_dict: dict[tuple[np.ndarray, np.ndarray]], train_city_list, validate_city_list, batch_size = 8):\n","    #     self.batch_size = batch_size\n","    #     # concatenate the training city's sequences into X, y\n","    #     trainX, trainy = [], []\n","    #     for city in train_city_list:\n","    #         cX, cy = city_seqs_dict[city]\n","    #         trainX.append(cX)\n","    #         trainy.append(cy)\n","    #     self.trainX: np.ndarray = np.concatenate(trainX, axis=0)\n","    #     self.trainy: np.ndarray = np.concatenate(trainy, axis=0)\n","    #     # concatenate the validation city's sequences into X, y\n","    #     validX, validy = [], []\n","    #     for city in validate_city_list:\n","    #         cX, cy = city_seqs_dict[city]\n","    #         validX.append(cX)\n","    #         validy.append(cy)\n","    #     self.validX: np.ndarray = np.concatenate(validX, axis=0)\n","    #     self.validy: np.ndarray = np.concatenate(validy, axis=0)\n","\n","    #     # make a dataloader out of these\n","    #     print(\"SimpleCityDataSet: concatenated train X.shape:\", self.trainX.shape)  # feature sequences\n","    #     print(\"SimpleCityDataSet: concatenated train y.shape:\", self.trainy.shape)  # labels\n","    #     print(\"SimpleCityDataSet: concatenated validate X.shape:\", self.validX.shape)  # feature sequences\n","    #     print(\"SimpleCityDataSet: concatenated validate y.shape:\", self.validy.shape)  # labels\n","\n","\n","    def __init__(self,\n","                 train_seqs_allX: np.ndarray, train_seqs_y: np.ndarray,\n","                 test_seqs_allX: np.ndarray, test_seqs_y: np.ndarray,\n","                 batch_size: int = 8):\n","        # assert train_seqs_allX.shape[0] == train_seqs_y.shape[0]\n","        # assert test_seqs_allX.shape[0] == test_seqs_y.shape[0]\n","        # assert train_seqs_allX.shape[1] == train_seqs_y.shape[1]\n","        # assert test_seqs_allX.shape[1] == test_seqs_y.shape[1]\n","        self.batch_size = batch_size\n","        self.trainX = train_seqs_allX  .astype(np.float32)\n","        self.trainy = train_seqs_y     .astype(np.float32)\n","        self.validX = test_seqs_allX   .astype(np.float32)\n","        self.validy = test_seqs_y      .astype(np.float32)\n","        # make a dataloader out of these\n","        print(\"SimpleCityDataSet: seqs train X.shape:\", self.trainX.shape)  # feature sequences\n","        print(\"SimpleCityDataSet: seqs train y.shape:\", self.trainy.shape)  # labels\n","        print(\"SimpleCityDataSet: seqs validate X.shape:\", self.validX.shape)  # feature sequences\n","        print(\"SimpleCityDataSet: seqs validate y.shape:\", self.validy.shape)  # labels\n","\n","\n","    # def concat_all_X(self):\n","    #     return np.concatenate((self.trainX, self.validX))\n","    # def concat_all_y(self):\n","    #     return np.concatenate((self.trainy, self.validy))\n","\n","\n","    def get_dataloader(self, train, last_batch = 'keep',\n","                       _batch_size_override = None, _dataloader_kwargs = {}) -> mx.gluon.data.DataLoader:\n","\n","        # [feature seqs, labels]\n","        if train=='both':\n","          shuffle = False\n","          tensors = [self.concat_all_X(), self.concat_all_y()]\n","        else:\n","          shuffle = bool(train)\n","          tensors = [self.trainX, self.trainy] if train else [self.validX, self.validy]\n","        dataset = mx.gluon.data.ArrayDataset(*tensors)  # *[feature seqs, labels]\n","        # shuffle = (self.shuffle if _shuffle_override is None else bool(_shuffle_override))\n","        batch_size = (self.batch_size if _batch_size_override is None else int(_batch_size_override))\n","        return mx.gluon.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, last_batch=last_batch, **_dataloader_kwargs)\n"]},{"cell_type":"markdown","metadata":{"id":"Pqulr2yj6QnJ"},"source":["# Complex (targetx, nbgrx, y) CityDataSet"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":150,"status":"aborted","timestamp":1701572400848,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"GWgeiuVS6UKs"},"outputs":[],"source":["class ComplexCityDataSet:\n","\n","    def __init__(self,\n","                 train_seqs_targetX: np.ndarray, train_seqs_neighborX: np.ndarray, train_seqs_y: np.ndarray,\n","                 test_seqs_targetX: np.ndarray, test_seqs_neighborX: np.ndarray, test_seqs_y: np.ndarray,\n","                 batch_size: int = 8):\n","        self.batch_size = batch_size\n","        self.traintgtX = train_seqs_targetX    .astype(np.float32)\n","        self.trainngbrX = train_seqs_neighborX .astype(np.float32)\n","        self.trainy = train_seqs_y             .astype(np.float32)\n","        self.validtgtX = test_seqs_targetX     .astype(np.float32)\n","        self.validngbrX = test_seqs_neighborX  .astype(np.float32)\n","        self.validy = test_seqs_y              .astype(np.float32)\n","        # make a dataloader out of these\n","        print(\"ComplexCityDataSet: seqs train target X.shape:\", self.traintgtX.shape)  # feature sequences\n","        print(\"ComplexCityDataSet: seqs train ngbr X.shape:\", self.trainngbrX.shape)  # feature sequences\n","        print(\"ComplexCityDataSet: seqs train y.shape:\", self.trainy.shape)  # labels\n","        print(\"ComplexCityDataSet: seqs validate target X.shape:\", self.validtgtX.shape)  # feature sequences\n","        print(\"ComplexCityDataSet: seqs validate ngbr X.shape:\", self.validngbrX.shape)  # feature sequences\n","        print(\"ComplexCityDataSet: seqs validate y.shape:\", self.validy.shape)  # labels\n","\n","\n","\n","    def get_dataloader(self, train, last_batch = 'keep',\n","                       _batch_size_override = None, _dataloader_kwargs = {}) -> mx.gluon.data.DataLoader:\n","\n","        # [feature seqs, labels]\n","        if train=='both':\n","          shuffle = False\n","          tensors = [self.concat_all_X(), self.concat_all_y()]\n","        else:\n","          shuffle = bool(train)\n","          tensors = [self.traintgtX, self.trainngbrX, self.trainy] if train \\\n","                        else [self.validtgtX, self.validngbrX, self.validy]\n","        dataset = mx.gluon.data.ArrayDataset(*tensors)  # *[feature seqs, labels]\n","        # shuffle = (self.shuffle if _shuffle_override is None else bool(_shuffle_override))\n","        batch_size = (self.batch_size if _batch_size_override is None else int(_batch_size_override))\n","        return mx.gluon.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, last_batch=last_batch, **_dataloader_kwargs)\n"]},{"cell_type":"markdown","metadata":{"id":"MU36yGJN1797"},"source":["# Utility functions (creating net, validation, graphing)"]},{"cell_type":"markdown","metadata":{"id":"Q_Vx0SCtt_It"},"source":["## create_net\n","\n","create_net(hidden_size=..., num_layers=...) defined here.\n","\n","Creates a Sequential net, of an LSTM and a Dense at the end\n","\n","The smaller number of layers was inspired by [this SO question/answer](https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1701572400849,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"vaIMqIx2uOdN"},"outputs":[],"source":["def create_net(hidden_size=32, num_layers=3, _out_dense_units=1):\n","\n","  ## create net\n","  net = nn.Sequential()\n","\n","  # use net's name_scope to give child Blocks appropriate names.\n","  with net.name_scope():\n","\n","    # LSTM\n","    #  default layout is 'TNC', but 'NTC' avoids having to transpose the tensors before passing through\n","    #   'TNC' corresponds to input tensor shape (seq_length, batch_size, num_inputs)\n","    # note: LSTM has arguments state_clip_min and state_clip_max but they aren't working on cpu mxnet\n","    # lstm_layer = rnn.LSTM(hidden_size=10, num_layers=3, layout='NTC')\n","    lstm_layer = rnn.LSTM(hidden_size=hidden_size, num_layers=num_layers, layout='NTC')\n","    net.add(lstm_layer)\n","\n","    # Dense:\n","    #  Inputs:  if flatten is True, data should be a tensor with shape (batch_size, x1, x2, ..., xn), where x1 * x2 * ... * xn is equal to in_units.\n","    #  Outputs:  if flatten is True, out will be a tensor with shape (batch_size, units).\n","    out_layer = nn.Dense(units=_out_dense_units, flatten=True)\n","    net.add(out_layer)\n","\n","  return net\n"]},{"cell_type":"markdown","metadata":{"id":"8ELTK2oa3S_b"},"source":["## save_net_to_file"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1701572400849,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"__pYBesb2K7a"},"outputs":[],"source":["_DRIVE_ROOT_FLDR = 'drive/MyDrive/Colab Notebooks Project/'\n","def save_net_to_file(net, filename, filefolder='params2'):\n","    # generate bash command arguments\n","    file_path = filefolder + '/' + filename\n","    repr_rel_folder = '.' if (not filefolder or len(filefolder)==0) else repr('./' + filefolder)\n","    # save\n","    net.save_parameters(file_path)\n","    # copy out to drive and back\n","    !cp {repr(file_path)} {repr(_DRIVE_ROOT_FLDR + file_path)}\n","    !cp {repr(_DRIVE_ROOT_FLDR + file_path)} {repr_rel_folder}\n","    print(f'saved parameters to \"{file_path}\"')"]},{"cell_type":"markdown","metadata":{"id":"IRZ4PBb32YUd"},"source":["## train with validation\n","```\n","train(model, trainer,\n","    train_dataloader, loss_fn,\n","    num_epochs, test_dataloader)\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1701572400850,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"DEcschI117Nt"},"outputs":[],"source":["## put testing into train function\n","\n","def train(model: nn.Block, trainer: gluon.Trainer, train_dataloader,\n","          loss_fn, num_epochs,\n","          test_dataloader=None,\n","          model_is_net2=False,\n","          save_every: int = 0, save_filename_template = 'filename_e{epoch}_tloss{tloss:.7f}_vloss{vloss:.7f}.params',\n","          save_folder: str = None, save_epoch_offset = 0,\n","          starting_best_vloss = 0,\n","          _gradient_clip = True,\n","          _verbosedbg=True):\n","    '''returns list of training losses for each epoch.\n","    <br>save_filename_template gets passed args `epoch`, `tloss`, `vloss`.'''\n","\n","    _do_saving = save_every > 0\n","    # _save_at = save_every - 1   # testing if _save_at == epoch % save_every\n","\n","    _lowest_vloss_yet = starting_best_vloss\n","    _do_vloss_save = False\n","    # _vloss = None; _tloss = None\n","\n","    train_losses = []\n","    test_losses = []\n","    train_start_time = time.time()\n","\n","    for epoch in range(num_epochs):\n","\n","        train_start_time = time.time()  # moved this from bottom to top of for-loop\n","\n","        # keep a sum for averaging this epoch's loss\n","        epoch_loss_sum = 0.\n","\n","        # Iterate over training data\n","        for idx, batchsandlabel in enumerate(train_dataloader):\n","\n","            if not model_is_net2:\n","                batch, label = batchsandlabel\n","\n","                # print(batch.shape)\n","                # print(batch)\n","\n","                if len(batch.shape) < 3:\n","                    batch = batch.reshape(*batch.shape, 1)  # shape so that the inputsize is 1\n","                batch = batch.astype('float32')\n","\n","                batch = batch.as_in_context(ctx)\n","                label = label.as_in_context(ctx)\n","\n","                batch_size = batch.shape[0]\n","\n","            else:\n","                targetbatch, ngbrbatch, label = batchsandlabel\n","\n","                # print(targetbatch.shape)\n","                # print(ngbrbatch.shape)\n","                # print(batch)\n","\n","                if len(targetbatch.shape) < 3:\n","                    targetbatch = targetbatch.reshape(*targetbatch.shape, 1)  # shape so that the inputsize is 1\n","                targetbatch = targetbatch.astype('float32')\n","                if len(ngbrbatch.shape) < 3:\n","                    ngbrbatch = ngbrbatch.reshape(*ngbrbatch.shape, 1)  # shape so that the inputsize is 1\n","                ngbrbatch = ngbrbatch.astype('float32')\n","\n","                targetbatch = targetbatch.as_in_context(ctx)\n","                ngbrbatch = ngbrbatch.as_in_context(ctx)\n","                label = label.as_in_context(ctx)\n","\n","                batch_size = targetbatch.shape[0]\n","\n","                if idx==0:\n","                  # perform one assertion just to ensure batch_size is same\n","                  # from both input batched x's\n","                  assert targetbatch.shape[0] == ngbrbatch.shape[0]\n","\n","\n","\n","\n","            with autograd.record():  # train_mode defaults to True\n","\n","                # print('debug: forward pass...')\n","\n","                ## Forward pass\n","                if not model_is_net2:\n","                    predicted = model(batch)\n","                else:\n","                    predicted = model(targetbatch, ngbrbatch)\n","\n","\n","                ## Compute loss\n","                loss: mx.numpy.ndarray = loss_fn(predicted, label)\n","\n","                if idx == 0:  # first run of loop\n","                  if _verbosedbg or (epoch == 0):  # print structure only once if verbose debug is False\n","                    # get a preview\n","                    # also set _params_prefix, used for debug messages later\n","                    print('weight params:', model.collect_params('.*weight'))\n","                    _params_prefix = model.collect_params('.*weight')._prefix\n","\n","\n","                ## my emergency debug stop if nan is found\n","                if math.isnan(loss[0]) \\\n","                            or any(math.isnan(i) for i in loss) \\\n","                            or math.isnan(model.collect_params('.*weight')[_params_prefix+'lstm0_l0_h2h_weight']._grad[0].min()):\n","                    print()\n","                    print(f'DEBUG force stopping at epoch {epoch} idx {idx}')\n","                    print(f'epoch_loss_sum: {epoch_loss_sum}')\n","                    print('loss:', loss)\n","                    print('train_losses array:', train_losses)\n","                    print('\\nDEBUG more info:')\n","                    print(f'batch: {batch}')\n","                    print(f'label: {label}')\n","                    print(f'predicted: {predicted}')\n","                    print(f'some weights data and grads, BEFORE loss.backward()  (the _grad is not updated yet):')\n","                    _i2h_weight = model.collect_params('.*weight')[_params_prefix+'lstm0_l0_i2h_weight']\n","                    _i2h_weight_data = _i2h_weight._data.copy()\n","                    _i2h_weight_grad = _i2h_weight._grad.copy()\n","                    print('  ',_i2h_weight)\n","                    print(f'    i2h _data shape: {_i2h_weight_data[0].shape}  |  [0]  min() {_i2h_weight_data[0].min()} , max() {_i2h_weight_data[0].max()}')\n","                    print(f'    i2h _grad shape: {_i2h_weight_grad[0].shape}  |  [0]  min() {_i2h_weight_grad[0].min()} , max() {_i2h_weight_grad[0].max()}')\n","                    _h2h_weight = model.collect_params('.*weight')[_params_prefix+'lstm0_l0_h2h_weight']\n","                    _h2h_weight_data = _h2h_weight._data.copy()\n","                    _h2h_weight_grad = _h2h_weight._grad.copy()\n","                    print('  ',_h2h_weight)\n","                    print(f'    h2h _data shape: {_h2h_weight_data[0].shape}  |  [0]  min() {_h2h_weight_data[0].min()} , max() {_h2h_weight_data[0].max()}')\n","                    print(f'    h2h _grad shape: {_h2h_weight_grad[0].shape}  |  [0]  min() {_h2h_weight_grad[0].min()} , max() {_h2h_weight_grad[0].max()}')\n","                    print(f' again, epoch {epoch} idx {idx}')\n","                    # exit to stop (the program keeps saying nan afterwards if you let it run)\n","                    raise RuntimeError(\"nan encountered. See debug info printed above this error\")\n","\n","\n","                # store loss\n","                epoch_loss_sum += float(loss.mean())\n","\n","                # print('debug: loss stored. Backward loss.backward() ...')\n","\n","                ## Backward pass  (gradients get updated)\n","                #  Note: The gradients didn't seem to update if this was outside autograd.record()\n","                #   which contradicts many examples\n","                loss.backward()\n","\n","\n","            # end autograd.record scope\n","\n","\n","            ## Optimize i.e. step the trainer\n","\n","            if not _gradient_clip:\n","                trainer.step(batch_size)\n","\n","            elif _gradient_clip:\n","                # attempt to perform gradient clipping\n","                if _verbosedbg: print(f' performing gradient clipping (epoch {epoch} idx {idx}, batch_size = batch.shape[0] = {batch_size})...')\n","                trainer.allreduce_grads()\n","                # https://github.com/apache/mxnet/issues/11508\n","                grads = [i.grad(ctx).as_nd_ndarray() for i in model.collect_params().values() if i._grad is not None]\n","                _total_norm = gluon.utils.clip_global_norm(arrays=grads, max_norm=1)\n","                if _verbosedbg: print(f'  debug: clip_global_norm\\'s total norm was {_total_norm}')\n","                # https://nlp.gluon.ai/examples/language_model/train_language_model.html\n","                trainer.update(batch_size=batch_size)\n","                # trainer.update(batch_size=1)\n","                # trainer.step(1)\n","\n","        # end for idx and batches (train)\n","\n","        train_elapsed = time.time() - train_start_time\n","\n","        train_losses.append(epoch_loss_sum/(idx+1))\n","\n","        ## start testing, if applicable\n","\n","        if test_dataloader:\n","\n","            test_start_time = time.time()\n","\n","            test_epoch_loss_sum = 0.\n","\n","            # Iterate over test data\n","            # for t_idx, (batch, label) in enumerate(test_dataloader):\n","            for t_idx, batchsandlabel in enumerate(train_dataloader):\n","\n","                if not model_is_net2:\n","                    batch, label = batchsandlabel\n","\n","                    if len(batch.shape) < 3:\n","                      batch = batch.reshape(*batch.shape, 1)  # shape so that the inputsize is 1\n","                    batch = batch.astype('float32')\n","\n","                    batch = batch.as_in_context(ctx)\n","                    label = label.as_in_context(ctx)\n","\n","                    batch_size = batch.shape[0]\n","\n","                else:\n","                    targetbatch, ngbrbatch, label = batchsandlabel\n","\n","                    # print(targetbatch.shape)\n","                    # print(ngbrbatch.shape)\n","                    # print(batch)\n","\n","                    if len(targetbatch.shape) < 3:\n","                        targetbatch = targetbatch.reshape(*targetbatch.shape, 1)  # shape so that the inputsize is 1\n","                    targetbatch = targetbatch.astype('float32')\n","                    if len(ngbrbatch.shape) < 3:\n","                        ngbrbatch = ngbrbatch.reshape(*ngbrbatch.shape, 1)  # shape so that the inputsize is 1\n","                    ngbrbatch = ngbrbatch.astype('float32')\n","\n","                    targetbatch = targetbatch.as_in_context(ctx)\n","                    ngbrbatch = ngbrbatch.as_in_context(ctx)\n","                    label = label.as_in_context(ctx)\n","\n","                    batch_size = targetbatch.shape[0]\n","\n","                    if idx==0:\n","                      # perform one assertion just to ensure batch_size is same\n","                      # from both input batched x's\n","                      assert targetbatch.shape[0] == ngbrbatch.shape[0]\n","\n","                ## Forward pass\n","                if not model_is_net2:\n","                    predicted = model(batch)\n","                else:\n","                    predicted = model(targetbatch, ngbrbatch)\n","\n","                ## Compute loss\n","                loss: mx.numpy.ndarray = loss_fn(predicted, label)\n","\n","                # store loss\n","                test_epoch_loss_sum += float(loss.mean())\n","\n","            # end for idx and batches (test)\n","\n","\n","            test_elapsed = time.time() - test_start_time\n","\n","            test_losses.append(test_epoch_loss_sum/(t_idx+1))\n","\n","        # end if test_dataloader\n","\n","        _test_epoch_loss_avg = 1000 if not test_dataloader else test_epoch_loss_sum/(t_idx+1)\n","\n","        print('epoch [{}/{}], loss: {:.7f} , {:.4f} sec'.format(\n","                  epoch + 1, num_epochs, epoch_loss_sum/(idx+1), train_elapsed)\n","              + ('' if not test_dataloader else ' | validate loss: {:.7f} , {:.4f} sec'.format(\n","                  _test_epoch_loss_avg, test_elapsed))\n","        )\n","\n","        if _test_epoch_loss_avg < _lowest_vloss_yet:\n","            _lowest_vloss_yet = _test_epoch_loss_avg\n","            _do_vloss_save = True\n","\n","        if ( _do_vloss_save ) or ( _do_saving and epoch > 0 and (epoch % save_every == 0) ):\n","            save_net_to_file(model, save_filename_template.format( epoch=epoch+save_epoch_offset, tloss=train_losses[-1], vloss=('nan' if not test_dataloader else test_losses[-1]) ), \\\n","                             **{'filefolder': p for p in [save_folder] if p is not None})  # extra optional kwarg\n","            _do_vloss_save = False\n","\n","    #end for epoch loop\n","\n","    print(f'train_losses: {train_losses}')\n","    if test_dataloader: print(f'test_losses: {test_losses}')\n","\n","    return train_losses if not test_dataloader else (train_losses, test_losses)"]},{"cell_type":"markdown","metadata":{"id":"Cxh98HA76g1a"},"source":["## plt_losses\n","\n","tlosses and vlosses would be lists of the losses\n","\n","plt_losses(tlosses, vlosses=None, split_axis=True, title=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1701572400850,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"rBdQrcAy6qFt"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as mtick\n","\n","def plt_losses(tlosses, vlosses=None, split_axis=True, title=None,\n","               x_offset=None):\n","    if not vlosses:\n","        split_axis=False\n","    axes2 = None\n","    plt.rcParams[\"figure.figsize\"] = (6.5,4.5)\n","    axes: plt.Axes = plt.gca()\n","    fig = axes.figure\n","    axes.set_xlabel('epoch indexes')\n","    if split_axis:\n","        axes2 = axes.twinx()\n","        axes.set_ylabel('training losses')\n","        axes2.set_ylabel('validation losses')\n","    else:\n","        axes.set_ylabel('losses')\n","\n","    # setup limits\n","    _tbottom=min(tlosses);  _ttop=max(tlosses)\n","    if vlosses: _vbottom=min(vlosses);  _vtop=max(vlosses)\n","    print('debug: testing:  tlosses min', min(tlosses));  print('debug: testing:  tlosses max', max(tlosses))\n","    if vlosses: print('debug: validate: vlosses min', min(vlosses));  print('debug: validate: vlosses max', max(vlosses))\n","    if split_axis:\n","        axes.set_ylim(bottom=_tbottom, top=_ttop)\n","        axes2.set_ylim(bottom=_vbottom, top=_vtop)\n","    else:\n","        if vlosses:\n","            axes.set_ylim(bottom=min(min(tlosses),min(vlosses)), top=max(max(tlosses),max(vlosses)))\n","        else:\n","            axes.set_ylim(bottom=min(tlosses), top=max(tlosses))\n","\n","    # plot\n","    xs = range(len(tlosses)) if x_offset is None else x_offset+np.arange(len(tlosses))\n","    tlines = axes.plot(xs, tlosses, color='blue', marker='o', linestyle='dotted', label='training')\n","    if vlosses:\n","        if not split_axis:\n","            axes2 = axes  # graph on same axes\n","        xs = range(len(vlosses)) if x_offset is None else x_offset+np.arange(len(vlosses))\n","        vlines = axes2.plot(xs, vlosses, color='orange', marker='+', linestyle='dashed', label='validation')\n","    del xs\n","\n","    # format ticks\n","    axes.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.4e'))\n","    if split_axis:\n","        axes2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.4e'))\n","\n","    # legend\n","    if split_axis:\n","        lines = tlines + vlines\n","        plt.legend(lines, [l.get_label() for l in lines])\n","    else:\n","        plt.legend()\n","    if title: plt.title(title)\n","    plt.show()\n","\n","# plt_losses(tlosses, vlosses, title='Split Y-axis Training and Validation')\n","# plt_losses(tlosses, vlosses, False, title='Common Y-axis Training and Validation')\n","\n","# plt_losses(tlosses, title='Training only (one y-axis)')"]},{"cell_type":"markdown","metadata":{"id":"NDA_z0vleOiA"},"source":["## secs_to_hhmmss (hh:mm:ss.ms)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087181,"status":"aborted","timestamp":1701572400853,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"AyLXY92peFJQ"},"outputs":[],"source":["def secs_to_hhmmss(seconds):\n","  return '{:02.0f}:{:02.0f}:{:02.3f}'.format(seconds//3600, seconds//60%60, seconds%60)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087179,"status":"aborted","timestamp":1701572400853,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"FI3z45VEeiE4"},"outputs":[],"source":["# testing  secs_to_hhmmss\n","_timediff = 9317.4914\n","print( '{:.4f}, {:.4f}, {:.4f}'.format(_timediff/3600, _timediff/60, _timediff) )\n","print( '{:.4f}, {:.4f}, {:.4f}'.format(_timediff/3600, _timediff/60%60, _timediff%3600) )\n","print( '{:02.0f}:{:02.0f}:{:02.3f}'.format(_timediff//3600, _timediff//60%60, _timediff%60) )\n","print( '{:02.0f}:{:02.0f}:{:02.3f}'.format((_timediff+1e6)//3600, (_timediff+1e6)//60%60, (_timediff+1e6)%60) )\n","print( secs_to_hhmmss(_timediff) )\n","print( secs_to_hhmmss(_timediff+1e6) )"]},{"cell_type":"markdown","metadata":{"id":"Rr0O_wu3_yyI"},"source":["## create_trainer, create_loss_fn\n","\n","create_trainer(net, type, lr, clip_grad)\n","\n","create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087178,"status":"aborted","timestamp":1701572400853,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"0ljrGNZg_7B9"},"outputs":[],"source":["def create_trainer(net, sgdtype='sgd', lr=0.1, clip_grad=5):\n","  '''if `clip_grad` param is passed `None`, then the clip_gradient arg will not be passed to gluon.Trainer'''\n","  if clip_grad is None:\n","    return gluon.Trainer(net.collect_params(), sgdtype, {'learning_rate': lr})\n","  return gluon.Trainer(net.collect_params(), sgdtype, {'learning_rate': lr, 'clip_gradient': clip_grad})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087176,"status":"aborted","timestamp":1701572400853,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"A1S1gi41_7eZ"},"outputs":[],"source":["def create_loss_fn():\n","  return gluon.loss.L2Loss()  # L2Loss is mean squared error (MSE)"]},{"cell_type":"markdown","metadata":{"id":"RkawC6j0lHNe"},"source":["Just calculate sqrt of MSE for analysis afterwards, same units."]},{"cell_type":"markdown","metadata":{"id":"r_gXpyNGZI5f"},"source":["# NN Creation"]},{"cell_type":"markdown","metadata":{"id":"9UK6fi2qZO5S"},"source":["## Simple one (process all together in LSTM)"]},{"cell_type":"markdown","metadata":{"id":"P7R8stjzfVP0"},"source":["### create_net_1\n","\n","create_net_1(hidden_size=..., num_layers=...) defined here.\n","\n","Creates a Sequential net, of an LSTM and a Dense at the end\n","\n","The smaller number of layers was inspired by [this SO question/answer](https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087173,"status":"aborted","timestamp":1701572400854,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"JXljDnX5fVP0"},"outputs":[],"source":["def create_net_1(hidden_size=32, num_layers=3, _out_dense_units=1):\n","\n","  ## create net\n","  net = nn.Sequential()\n","\n","  # use net's name_scope to give child Blocks appropriate names.\n","  with net.name_scope():\n","\n","    # LSTM\n","    #  default layout is 'TNC', but 'NTC' avoids having to transpose the tensors before passing through\n","    #   'TNC' corresponds to input tensor shape (seq_length, batch_size, num_inputs)\n","    # note: LSTM has arguments state_clip_min and state_clip_max but they aren't working on cpu mxnet\n","    # lstm_layer = rnn.LSTM(hidden_size=10, num_layers=3, layout='NTC')\n","    lstm_layer = rnn.LSTM(hidden_size=hidden_size, num_layers=num_layers, layout='NTC')\n","    net.add(lstm_layer)\n","\n","    # Dense:\n","    #  Inputs:  if flatten is True, data should be a tensor with shape (batch_size, x1, x2, ..., xn), where x1 * x2 * ... * xn is equal to in_units.\n","    #  Outputs:  if flatten is True, out will be a tensor with shape (batch_size, units).\n","    out_layer = nn.Dense(units=_out_dense_units, flatten=True)\n","    net.add(out_layer)\n","\n","  return net"]},{"cell_type":"markdown","metadata":{"id":"vp1I92r1ZRFQ"},"source":["## Complex one (processing neighbors before LSTM)"]},{"cell_type":"markdown","metadata":{"id":"h3S6v9lKfYuH"},"source":["### create_net_2\n","\n","create_net_2(hidden_size=..., num_layers=..., ...) defined here.\n","\n","Creates a Net to process Neighbors, then concat, then LSTM and a Dense at the end\n","\n","The smaller number of layers was inspired by [this SO question/answer](https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087172,"status":"aborted","timestamp":1701572400854,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"NyX1awcpfYuO"},"outputs":[],"source":["## net\n","  #   target x\n","  # Dense\n","  # Dense\n","  #   Concat target x, neighbor x\n","  # LSTM\n","  # Dense\n","  #   out\n","\n","from mxnet.gluon import Block\n","from mxnet import ndarray as F\n","# from mxnet import nd\n","\n","class Net2NeighborPart(Block):\n","\n","  def __init__(self, inner_dense_sizes, out_units, **kwargs):\n","    super(Net2NeighborPart, self).__init__(**kwargs)\n","\n","    with self.name_scope():\n","      self.dense0 = nn.Dense(inner_dense_sizes[0],flatten=False) # for example, 32 units (intermediary)\n","      self.denseout = nn.Dense(out_units,flatten=False)    # for example, 9 units out\n","\n","  def forward(self, x):\n","    x = self.dense0(x)\n","    # x = F.relu(self.dense0(x))\n","    return self.denseout(x)\n","\n","class Net2(Block):\n","\n","  def __init__(self, hidden_size, num_layers, out_dense_units,\n","               ngbr_inner_dense_sizes, ngbr_out_dense_units, **kwargs):\n","    super(Net2, self).__init__(**kwargs)\n","\n","    with self.name_scope():\n","      self.neighbor_net = Net2NeighborPart(inner_dense_sizes = ngbr_inner_dense_sizes,\n","                                            out_units = ngbr_out_dense_units)\n","\n","      # LSTM\n","      #  default layout is 'TNC', but 'NTC' avoids having to transpose the tensors before passing through\n","      #   'TNC' corresponds to input tensor shape (seq_length, batch_size, num_inputs)\n","      # note: LSTM has arguments state_clip_min and state_clip_max but they aren't working on cpu mxnet\n","      # lstm_layer = rnn.LSTM(hidden_size=10, num_layers=3, layout='NTC')\n","      self.lstm0 = rnn.LSTM(hidden_size=hidden_size, num_layers=num_layers, layout='NTC')\n","\n","      self.densefinal = nn.Dense(out_dense_units)\n","\n","\n","  def forward(self, targetX, ngbrX):\n","    ngbrX = self.neighbor_net(ngbrX)\n","\n","    # concat to make the LSTM input all together now\n","\n","    x = F.np.concatenate((targetX, ngbrX),axis=2)\n","\n","    x = self.lstm0(x)\n","\n","    return self.densefinal(x)\n","\n","\n","def create_net_2(hidden_size=32, num_layers=3, _out_dense_units=1,\n","                 _ngbr_inner_dense_sizes=[32], _ngbr_out_dense_units=9):\n","\n","  net = Net2(hidden_size = hidden_size, num_layers = num_layers,\n","             out_dense_units = _out_dense_units,\n","             ngbr_inner_dense_sizes = _ngbr_inner_dense_sizes,\n","             ngbr_out_dense_units = _ngbr_out_dense_units)\n","\n","  #   # LSTM\n","  #   #  default layout is 'TNC', but 'NTC' avoids having to transpose the tensors before passing through\n","  #   #   'TNC' corresponds to input tensor shape (seq_length, batch_size, num_inputs)\n","  #   # note: LSTM has arguments state_clip_min and state_clip_max but they aren't working on cpu mxnet\n","  #   # lstm_layer = rnn.LSTM(hidden_size=10, num_layers=3, layout='NTC')\n","  #   lstm_layer = rnn.LSTM(hidden_size=hidden_size, num_layers=num_layers, layout='NTC')\n","  #   net.add(lstm_layer)\n","\n","  #   # Dense:\n","  #   #  Inputs:  if flatten is True, data should be a tensor with shape (batch_size, x1, x2, ..., xn), where x1 * x2 * ... * xn is equal to in_units.\n","  #   #  Outputs:  if flatten is True, out will be a tensor with shape (batch_size, units).\n","  #   out_layer = nn.Dense(units=_out_dense_units, flatten=True)\n","  #   net.add(out_layer)\n","\n","  return net\n"]},{"cell_type":"markdown","metadata":{"id":"VN_C9pzccAQw"},"source":["# Random Forest Tree (to be baseline comparison)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087326,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"HRPx3VBsb_tI"},"outputs":[],"source":["# SEE OTHER FILE FOR Random Forest Tree"]},{"cell_type":"markdown","metadata":{"id":"Ewty7cUS7A_m"},"source":["# Running tests for Simple"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087324,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"N7TdtohRFU4f"},"outputs":[],"source":["print(together_train_y_seqs.shape)\n","print(together_train_allx_seqs.shape)\n","print(together_test_y_seqs.shape)\n","print(together_test_allx_seqs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087323,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"IgwKmB7rEj6K"},"outputs":[],"source":["data = SimpleCityDataSet(together_train_allx_seqs, together_train_y_seqs,\n","                            together_test_allx_seqs, together_test_y_seqs)\n","print(data)\n","dataloader = data.get_dataloader(train=True)\n","_firstitem = next(iter(dataloader))\n","print('first item of dataloader:')\n","print(_firstitem)\n","print('expecting 2 items:', len(_firstitem))\n","print('shapes:')\n","print(_firstitem[0].shape, _firstitem[1].shape)\n","print('should be shaped like (batch_size, seq_length, num_imputs)')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087321,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"FcXnBYFKO2gC"},"outputs":[],"source":["data = SimpleCityDataSet(together_train_allx_seqs, together_train_y_seqs,\n","                            together_test_allx_seqs, together_test_y_seqs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087320,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"MaRz7aYcnGbb"},"outputs":[],"source":["print(together_train_y_seqs.shape)\n","print(together_train_y_seqs[:,0,:].shape)\n","print(together_train_y_seqs[0].shape)\n","print(together_train_y_seqs[0])\n","print(together_train_y_seqs[:,0,:])\n","print('Note: the start of the sequences [:, 0, :] are not guaranteed to be truly consecutive'\\\n","      +'\\n because some sequences during preparation were cut out due to the data being non-consecutive')"]},{"cell_type":"markdown","metadata":{"id":"96anUUdjdolj"},"source":["## hidden size 32 (see function definition for reason!), num_layers 3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087319,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"O5MAC_W3688N"},"outputs":[],"source":["net = create_net_1(hidden_size=32, num_layers=3)\n","\n","net.initialize(mx.init.Xavier(), ctx=ctx)\n","\n","print('net:');  print(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087317,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Pgam7NECoWkj"},"outputs":[],"source":["print(data)\n","train_dataloader: gluon.data.DataLoader = data.get_dataloader(train=True)\n","val_dataloader: gluon.data.DataLoader = data.get_dataloader(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087315,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"KcLxkrh1e_VP"},"outputs":[],"source":["## trainer for net\n","trainer = create_trainer(net)\n","loss_fn = create_loss_fn()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087314,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"GBROPEJHdVkv"},"outputs":[],"source":["## train, test\n","_stime = time.time()\n","tlosses, vlosses = train(net, trainer, train_dataloader, loss_fn, num_epochs=40, test_dataloader=val_dataloader, _verbosedbg=False)\n","_timediff = time.time() - _stime\n","print('total time diff for train() call, all epochs: {:.4f} sec (hh:mm:ss {})'.format(_timediff, secs_to_hhmmss(_timediff)))\n","\n","# save net params in case Google Colab session ends\n","file_name = \"params2/test_net_1.params\"\n","net.save_parameters(file_name)\n","# copy out to drive and back\n","!cp 'params2/test_net_1.params' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1.params'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1.params' ./params2\n","print(f'saved parameters to \"{file_name}\"')\n","\n","# plot losses\n","plt_losses(tlosses, vlosses)\n","plt_losses(tlosses, vlosses, split_axis=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087312,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"K_wwgsgRzNuf"},"outputs":[],"source":["net = create_net_1(hidden_size=32, num_layers=3)\n","net.initialize(mx.init.Xavier(), ctx=ctx)\n","print('net:');  print(net)\n","## trainer for net\n","trainer = create_trainer(net)\n","loss_fn = create_loss_fn()\n","\n","## train, test\n","_stime = time.time()\n","tlosses, vlosses = train(net, trainer, train_dataloader, loss_fn, num_epochs=10, test_dataloader=val_dataloader, _verbosedbg=False)\n","_timediff = time.time() - _stime\n","print('total time diff for train() call, all epochs: {:.4f} sec (hh:mm:ss {})'.format(_timediff, secs_to_hhmmss(_timediff)))\n","# plot losses\n","plt_losses(tlosses, vlosses)\n","plt_losses(tlosses, vlosses, split_axis=False)\n"]},{"cell_type":"markdown","metadata":{"id":"VrTdbV3QdvVA"},"source":["### again"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087311,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"_FiFePB3d3F1"},"outputs":[],"source":["net = create_net_1(hidden_size=32, num_layers=3)\n","net.initialize(mx.init.Xavier(), ctx=ctx)\n","print('net:');  print(net)\n","## trainer for net\n","trainer = create_trainer(net)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087310,"status":"aborted","timestamp":1701572401010,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"XSDOzkTTd5Fg"},"outputs":[],"source":["## train, test\n","_stime = time.time()\n","tlosses, vlosses = train(net, trainer, train_dataloader, loss_fn, num_epochs=50, test_dataloader=val_dataloader, _verbosedbg=False)\n","_timediff = time.time() - _stime\n","print('total time diff for train() call, all epochs: {:.4f} sec (hh:mm:ss {})'.format(_timediff, secs_to_hhmmss(_timediff)))\n","\n","# save net params in case Google Colab session ends\n","file_name = \"params2/test_net_1_e{epoch}_tl{tloss:.4f}_vl{vloss:.4f}.params\".format(\n","    epoch=len(tlosses), tloss=tlosses[-1], vloss=vlosses[-1]\n",")\n","net.save_parameters(file_name)\n","# copy out to drive and back\n","!cp {repr(file_name)} {repr('drive/MyDrive/Colab Notebooks Project/'+file_name)}\n","!cp {repr('drive/MyDrive/Colab Notebooks Project/'+file_name)} ./params2\n","print(f'saved parameters to \"{file_name}\"')\n","# and as ... _latest .params\n","!cp {repr(file_name)} 'params2/test_net_1_latest.params'\n","!cp 'params2/test_net_1_latest.params' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_latest.params'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_latest.params' ./params2\n","print(f'copied parameters to \"params2/test_net_1_latest.params\"')\n","\n","# plot losses\n","plt_losses(tlosses, vlosses)\n","plt_losses(tlosses, vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087309,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"rdyGzFf8eHtA"},"outputs":[],"source":["# save old losses\n","with open('params2/test_net_1_losses.json', 'w') as f:\n","  json.dump({'train_losses':tlosses, 'test_losses':vlosses}, f)\n","!cp 'params2/test_net_1_losses.json' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_losses.json'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_losses.json' ./params2"]},{"cell_type":"markdown","metadata":{"id":"a1dJBTJfd0jo"},"source":["### again! load from saved"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087308,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"2oxFIhKgdy_E"},"outputs":[],"source":["!cp -r 'drive/MyDrive/Colab Notebooks Project/params2/' .\n","# open previous losses\n","with open('params2/test_net_1_losses.json', 'r') as f:\n","  _lossdict = json.load(f)\n","prev_tlosses = _lossdict['train_losses'] #['tlosses']\n","prev_vlosses = _lossdict['test_losses'] #['vlosses']\n","del _lossdict"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087307,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"3XjdBwtyeKgD"},"outputs":[],"source":["print(prev_tlosses); print(prev_vlosses)\n","print(len(prev_tlosses), 'previous training losses')\n","print(len(prev_vlosses), 'previous testing losses')\n","plt_losses(prev_tlosses, prev_vlosses, split_axis=False)\n","if len(prev_tlosses) >= 100:\n","    plt_losses(prev_tlosses[-100:], prev_vlosses[-100:], split_axis=False,\n","               x_offset=len(prev_tlosses)-100,\n","               title='Last 100 losses')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087305,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"MJElvVsmez5z"},"outputs":[],"source":["net = create_net_1(hidden_size=32, num_layers=3)\n","print('net:');  print(net)\n","# reload from saved params\n","net.load_parameters('params2/test_net_1_latest.params', ctx=ctx)\n","print('net:');  print(net)\n","\n","## trainer for net\n","trainer = create_trainer(net)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087304,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"VJT6wM-Jvv80"},"outputs":[],"source":["train_dataloader: gluon.data.DataLoader = data.get_dataloader(train=True)\n","val_dataloader: gluon.data.DataLoader = data.get_dataloader(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087302,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"-OtMqX7OfcRV"},"outputs":[],"source":["_epoch_offset = len(prev_tlosses)+1\n","print('epoch offset used: ', _epoch_offset)\n","## train, test\n","_stime = time.time()\n","# tlosses, vlosses = train(net, trainer, train_dataloader, loss_fn, num_epochs=25, test_dataloader=val_dataloader, _verbosedbg=False)\n","tlosses, vlosses = train(net, trainer, train_dataloader, loss_fn, num_epochs=50, test_dataloader=val_dataloader,\n","                         save_every=10,\n","                         save_epoch_offset=_epoch_offset,\n","                         save_filename_template='test_net_2_e{epoch}_tl{tloss:.4f}_vl{vloss:.4f}.params',\n","                         save_folder = 'params2',\n","                         starting_best_vloss = min(prev_vlosses),  # <----  also saving when new best validation loss achieved\n","                         _verbosedbg=False)\n","_timediff = time.time() - _stime\n","print('total time diff for train() call, all epochs: {:.4f} sec (hh:mm:ss {})'.format(_timediff, secs_to_hhmmss(_timediff)))\n","\n","# save net params in case Google Colab session ends\n","file_name = \"params2/test_net_1_e{epoch}_tl{tloss:.4f}_vl{vloss:.4f}.params\".format(\n","    epoch=len(prev_tlosses)+len(tlosses), tloss=tlosses[-1], vloss=vlosses[-1]\n",")\n","net.save_parameters(file_name)\n","# copy out to drive and back\n","!cp {repr(file_name)} {repr('drive/MyDrive/Colab Notebooks Project/'+file_name)}\n","!cp {repr('drive/MyDrive/Colab Notebooks Project/'+file_name)} ./params2\n","print(f'saved parameters to \"{file_name}\"')\n","# and as ... _latest .params\n","!cp {repr(file_name)} 'params2/test_net_1_latest.params'\n","!cp 'params2/test_net_1_latest.params' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_latest.params'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_latest.params' ./params2\n","print(f'copied parameters to \"params2/test_net_1_latest.params\"')\n","\n","# plot losses\n","plt_losses(prev_tlosses + tlosses, prev_vlosses + vlosses)\n","plt_losses(prev_tlosses + tlosses, prev_vlosses + vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087301,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"EDAl7RqnTri1"},"outputs":[],"source":["# plt_losses(tlosses[:50]+tlosses[100:], vlosses[:50]+vlosses[100:], split_axis=False)\n","# plt_losses(tlosses, vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087300,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Cfj7ykuMT2BG"},"outputs":[],"source":["# tlosses = tlosses[:50] + tlosses[100:]\n","# vlosses = vlosses[:50] + vlosses[100:]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087299,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"ZJqblkIDUC2p"},"outputs":[],"source":["print(len(tlosses))\n","print(len(prev_tlosses))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087298,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"P127AMS8Fql8"},"outputs":[],"source":["if len(prev_tlosses) >= 100:\n","    plt_losses(prev_tlosses[-100:], prev_vlosses[-100:], split_axis=False,\n","               x_offset=len(prev_tlosses)-100,\n","               title='Last 100 losses')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087297,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Cjfjyd8yeKLi"},"outputs":[],"source":["# combine old with new losses\n","tlosses = prev_tlosses + tlosses\n","vlosses = prev_vlosses + vlosses\n","# print how many now\n","print(len(tlosses), 'now total training losses')\n","print(len(vlosses), 'now total testing losses')\n","# save losses\n","with open('params2/test_net_1_losses.json', 'w') as f:\n","  json.dump({'train_losses':tlosses, 'test_losses':vlosses}, f)\n","!cp 'params2/test_net_1_losses.json' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_losses.json'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_1_losses.json' ./params2"]},{"cell_type":"markdown","metadata":{"id":"JE9ZH9mt4n6K"},"source":["# Running tests for Complex"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087294,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"U2le9E7B5GEA"},"outputs":[],"source":["print(separate_train_y_seqs.shape)\n","print(separate_train_targetx_seqs.shape)\n","print(separate_train_neighborx_seqs.shape)\n","print(separate_test_y_seqs.shape)\n","print(separate_test_targetx_seqs.shape)\n","print(separate_test_neighborx_seqs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087293,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"YlQBH_yl5GEB"},"outputs":[],"source":["data2 = ComplexCityDataSet(separate_train_targetx_seqs, separate_train_neighborx_seqs, separate_train_y_seqs,\n","                            separate_test_targetx_seqs, separate_test_neighborx_seqs, separate_test_y_seqs)\n","print(data2)\n","dataloader2 = data2.get_dataloader(train=True)\n","_firstitem = next(iter(dataloader2))\n","print('first item of dataloader2:')\n","print(_firstitem)\n","print('expecting 3 items:', len(_firstitem))\n","print('shapes:')\n","print(_firstitem[0].shape, _firstitem[1].shape, _firstitem[2].shape)\n","print('should be shaped like (batch_size, seq_length, num_imputs) ; third one is the y')"]},{"cell_type":"markdown","metadata":{"id":"VEsaC-ae5EVA"},"source":["## net2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087291,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"lgBvx2zs4mcq"},"outputs":[],"source":["net2 = create_net_2(hidden_size=32, num_layers=3,\n","                    _ngbr_inner_dense_sizes=[32], _ngbr_out_dense_units=9)\n","net2.initialize(mx.init.Xavier(), ctx=ctx)\n","print('net2:');  print(net2)\n","## trainer for net\n","trainer = create_trainer(net2)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087285,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"WGGgdRNo56_g"},"outputs":[],"source":["train_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=True)\n","val_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087284,"status":"aborted","timestamp":1701572401011,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"NpZyCW-s4hq6"},"outputs":[],"source":["## train, test\n","_stime = time.time()\n","tlosses, vlosses = train(net2, trainer, train_dataloader2, loss_fn, num_epochs=50, test_dataloader=val_dataloader2,\n","                         model_is_net2=True,  # <----\n","                         _verbosedbg=False)\n","_timediff = time.time() - _stime\n","print('total time diff for train() call, all epochs: {:.4f} sec (hh:mm:ss {})'.format(_timediff, secs_to_hhmmss(_timediff)))\n","\n","# save net params in case Google Colab session ends\n","file_name = \"params2/test_net_2.params\"\n","net2.save_parameters(file_name)\n","# copy out to drive and back\n","!cp 'params2/test_net_2.params' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2.params'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2.params' ./params2\n","print(f'saved parameters to \"{file_name}\"')\n","\n","# plot losses\n","plt_losses(tlosses, vlosses)\n","plt_losses(tlosses, vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087284,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"EsG__-ElfMgo"},"outputs":[],"source":["# save old losses\n","with open('params2/test_net_2_losses.json', 'w') as f:\n","  json.dump({'train_losses':tlosses, 'test_losses':vlosses})\n","!cp 'params2/test_net_2_losses.json' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_losses.json'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_losses.json' ./params2"]},{"cell_type":"markdown","metadata":{"id":"UK3xLpvOcNU1"},"source":["### reload it from its saved file"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087282,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"1yTq2n2AcMjb"},"outputs":[],"source":["!cp -r 'drive/MyDrive/Colab Notebooks Project/params2/' .\n","# open previous losses\n","with open('params2/test_net_2_losses.json', 'r') as f:\n","  _lossdict = json.load(f)\n","prev_tlosses = _lossdict['train_losses'] #['tlosses']\n","prev_vlosses = _lossdict['test_losses'] #['vlosses']\n","del _lossdict"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087281,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"0XVCdRNF-z2a"},"outputs":[],"source":["print(prev_tlosses); print(prev_vlosses)\n","print(len(prev_tlosses), 'previous training losses')\n","print(len(prev_vlosses), 'previous testing losses')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087280,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"W2EOHqlBgaVQ"},"outputs":[],"source":["net2 = create_net_2(hidden_size=32, num_layers=3,\n","                    _ngbr_inner_dense_sizes=[32], _ngbr_out_dense_units=9)\n","# net2.initialize(mx.init.Xavier(), ctx=ctx)\n","print('net2:');  print(net2)\n","# reload from saved params\n","net2.load_parameters('params2/test_net_2.params', ctx=ctx)\n","print('net2:');  print(net2)\n","\n","## trainer for net\n","trainer = create_trainer(net2)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087279,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"PvtW90E_gv0T"},"outputs":[],"source":["train_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=True)\n","val_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087278,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"T4jqzZzjg84E"},"outputs":[],"source":["## train, test\n","_stime = time.time()\n","tlosses, vlosses = train(net2, trainer, train_dataloader2, loss_fn, num_epochs=50, test_dataloader=val_dataloader2,\n","                         model_is_net2=True,  # <----\n","                         _verbosedbg=False)\n","_timediff = time.time() - _stime\n","print('total time diff for train() call, all epochs: {:.4f} sec (hh:mm:ss {})'.format(_timediff, secs_to_hhmmss(_timediff)))\n","\n","# save net params in case Google Colab session ends\n","file_name = \"params2/test_net_2.params\"\n","net2.save_parameters(file_name)\n","# copy out to drive and back\n","!cp 'params2/test_net_2.params' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2.params'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2.params' ./params2\n","print(f'saved parameters to \"{file_name}\"')\n","\n","# plot losses\n","plt_losses(prev_tlosses + tlosses, prev_vlosses + vlosses)\n","plt_losses(prev_tlosses + tlosses, prev_vlosses + vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087276,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"DFW5yppw_n6c"},"outputs":[],"source":["# combine old with new losses\n","tlosses = prev_tlosses + tlosses\n","vlosses = prev_vlosses + vlosses\n","\n","print(len(tlosses), 'now total training losses')\n","print(len(vlosses), 'now total testing losses')\n","\n","# save losses\n","with open('params2/test_net_2_losses.json', 'w') as f:\n","  json.dump({'train_losses':tlosses, 'test_losses':vlosses}, f)\n","!cp 'params2/test_net_2_losses.json' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_losses.json'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_losses.json' ./params2"]},{"cell_type":"markdown","metadata":{"id":"svMrw8CAP5QQ"},"source":["### again! more!"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087275,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"ZrGEzbftP3ZU"},"outputs":[],"source":["# again!\n","!cp -r 'drive/MyDrive/Colab Notebooks Project/params2/' .\n","# open previous losses\n","with open('params2/test_net_2_losses.json', 'r') as f:\n","  _lossdict = json.load(f)\n","prev_tlosses = _lossdict['train_losses']\n","prev_vlosses = _lossdict['test_losses']\n","del _lossdict\n","\n","print(prev_tlosses); print(prev_vlosses)\n","print(len(prev_tlosses), 'previous training losses')\n","print(len(prev_vlosses), 'previous testing losses')\n","\n","print('OLD (PREVIOUS) LOSSES:')\n","plt_losses(prev_tlosses, prev_vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087274,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Xs_kVao0Rvxe"},"outputs":[],"source":["net2 = create_net_2(hidden_size=32, num_layers=3,\n","                    _ngbr_inner_dense_sizes=[32], _ngbr_out_dense_units=9)\n","print('net2:');  print(net2)\n","# reload from saved params\n","net2.load_parameters('params2/test_net_2_latest.params', ctx=ctx)\n","print('net2:');  print(net2)\n","\n","## trainer for net\n","trainer = create_trainer(net2)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087273,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"7yVc2WxrR_Q0"},"outputs":[],"source":["train_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=True)\n","val_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087271,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"S6KRkSfsRzpS"},"outputs":[],"source":["_epoch_offset = len(prev_tlosses)+1\n","print('epoch offset used: ', _epoch_offset)\n","## train, test\n","_stime = time.time()\n","tlosses, vlosses = train(net2, trainer, train_dataloader2, loss_fn, num_epochs=100, test_dataloader=val_dataloader2,\n","                         model_is_net2=True,  # <----\n","                         save_every=10,\n","                         save_epoch_offset=_epoch_offset,\n","                         save_filename_template='test_net_2_e{epoch}_tl{tloss:.4f}_vl{vloss:.4f}.params',\n","                         save_folder = 'params2',\n","                         starting_best_vloss = min(prev_vlosses),  # <----  also saving when new best validation loss achieved\n","                         _verbosedbg=False)\n","_timediff = time.time() - _stime\n","print('total time diff for train() call, all epochs: {:.4f} sec (hh:mm:ss {})'.format(_timediff, secs_to_hhmmss(_timediff)))\n","\n","# save net params in case Google Colab session ends\n","file_name = \"params2/test_net_2_e{epoch}_tl{tloss:.4f}_vl{vloss:.4f}.params\".format(\n","    epoch=len(prev_tlosses)+len(tlosses), tloss=tlosses[-1], vloss=vlosses[-1]\n",")\n","net2.save_parameters(file_name)\n","# copy out to drive and back\n","!cp {repr(file_name)} {repr('drive/MyDrive/Colab Notebooks Project/'+file_name)}\n","!cp {repr('drive/MyDrive/Colab Notebooks Project/'+file_name)} ./params2\n","print(f'saved parameters to \"{file_name}\"')\n","# and as ... _latest .params\n","!cp {repr(file_name)} 'params2/test_net_2_latest.params'\n","!cp 'params2/test_net_2_latest.params' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_latest.params'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_latest.params' ./params2\n","print(f'copied parameters to \"params2/test_net_2_latest.params\"')\n","\n","# plot losses\n","plt_losses(prev_tlosses + tlosses, prev_vlosses + vlosses)\n","plt_losses(prev_tlosses + tlosses, prev_vlosses + vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087270,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"KIUpBXqBUWGz"},"outputs":[],"source":["# combine old with new losses\n","tlosses = prev_tlosses + tlosses\n","vlosses = prev_vlosses + vlosses\n","# print how many\n","print(len(tlosses), 'now total training losses')\n","print(len(vlosses), 'now total testing losses')\n","# SAVE all losses\n","with open('params2/test_net_2_losses.json', 'w') as f:\n","  json.dump({'train_losses':tlosses, 'test_losses':vlosses}, f)\n","!cp 'params2/test_net_2_losses.json' 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_losses.json'\n","!cp 'drive/MyDrive/Colab Notebooks Project/params2/test_net_2_losses.json' ./params2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087269,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Ri2IxSlrpT8h"},"outputs":[],"source":["if len(tlosses) >= 100:\n","    plt_losses(tlosses[-100:], vlosses[-100:], split_axis=False,\n","               x_offset=len(tlosses)-100,\n","               title='Last 100 t&v losses')"]},{"cell_type":"markdown","metadata":{"id":"Hjy7wDDREavS"},"source":["# Graph predictions over real"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087267,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"UrdvnWnH7URq"},"outputs":[],"source":["# again!\n","!cp -r 'drive/MyDrive/Colab Notebooks Project/params2/' .\n","# open previous losses\n","with open('params2/test_net_2_losses.json', 'r') as f:\n","  _lossdict = json.load(f)\n","prev_tlosses = _lossdict['train_losses']\n","prev_vlosses = _lossdict['test_losses']\n","del _lossdict\n","\n","print(prev_tlosses); print(prev_vlosses)\n","print(len(prev_tlosses), 'previous training losses')\n","print(len(prev_vlosses), 'previous testing losses')\n","\n","print('OLD (PREVIOUS) LOSSES:')\n","plt_losses(prev_tlosses, prev_vlosses, split_axis=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087266,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Vwjr4g417X4b"},"outputs":[],"source":["net2 = create_net_2(hidden_size=32, num_layers=3,\n","                    _ngbr_inner_dense_sizes=[32], _ngbr_out_dense_units=9)\n","print('net2:');  print(net2)\n","# reload from saved params\n","net2.load_parameters('params2/test_net_2_latest.params', ctx=ctx)\n","print('net2:');  print(net2)\n","\n","## trainer for net\n","trainer = create_trainer(net2)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087265,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"J1oAcCRWEaSU"},"outputs":[],"source":["tcc = train_citycode_list[0]\n","vcc = test_citycode_list[0]\n","print('train city:', tcc, cities_names_dict[tcc])\n","print('valid city:', vcc, cities_names_dict[vcc])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087264,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"cUr_q7QvAVTb"},"outputs":[],"source":["print(len(tlosses))"]},{"cell_type":"markdown","metadata":{"id":"ttZc3XR87Z5b"},"source":["# RMSE, MAE, R2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087262,"status":"aborted","timestamp":1701572401012,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"eajgQrE9IQmX"},"outputs":[],"source":["plt.rcParams['font.family'] = 'serif'\n","plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n","plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087262,"status":"aborted","timestamp":1701572401013,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"vmt2m8hmDga8"},"outputs":[],"source":["!cp 'drive/MyDrive/Colab Notebooks Project/presequencing_means_stds_dict.json' .\n","with open('presequencing_means_stds_dict.json', 'r') as f:\n","  means_stds_dict = json.load(f)\n","print(means_stds_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087260,"status":"aborted","timestamp":1701572401013,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"C-pmOk_3LcpJ"},"outputs":[],"source":["# unstandardize PM2.5\n","def _unstdize(ys):\n","  _pm25mean, _pm25std = means_stds_dict['PM25']\n","  return (ys*_pm25std)+_pm25mean"]},{"cell_type":"markdown","metadata":{"id":"D1j_2abSyfyU"},"source":["## function for R2 from reals and preds\n","SSE = n * MSE,\n","or MSE = 1/n * SSE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087259,"status":"aborted","timestamp":1701572401013,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"0qh-j-ng7dSx"},"outputs":[],"source":["def calc_r2(real_ys, pred_ys):\n","  print(f'  real_ys.shape: {real_ys.shape} ; pred_ys.shape: {pred_ys.shape}')\n","  _mean_real_y = real_ys.mean()\n","  _TSS = np.square((real_ys - _mean_real_y)).sum();  # total sum of squares\n","  _SSE = np.square((real_ys - pred_ys)).sum();  # sum of squared errors\n","  _R2 = 1 - _SSE / _TSS\n","  print(f'  mean real y = {_mean_real_y}')\n","  print(f'  TSS = {_TSS}')\n","  print(f'  SSE = {_SSE}')\n","  print(f'    MSE = {_SSE / len(pred_ys)}')\n","  print(f'    RMSE = {np.sqrt(_SSE / len(pred_ys))}')\n","  print(f' R2 = SSE / TSS = {_R2}')\n","  return _R2"]},{"cell_type":"markdown","metadata":{"id":"BuFG0eNM0LCK"},"source":["## net1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087258,"status":"aborted","timestamp":1701572401013,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"boyzaZAF0KV3"},"outputs":[],"source":["data = SimpleCityDataSet(together_train_allx_seqs, together_train_y_seqs,\n","                            together_test_allx_seqs, together_test_y_seqs)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087417,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"cyS9P8qG0M_4"},"outputs":[],"source":["net = create_net_1(hidden_size=32, num_layers=3)\n","print('net:');  print(net)\n","# reload from saved params\n","net.load_parameters('params2/test_net_1_latest.params', ctx=ctx)\n","print('net:');  print(net)\n","\n","## trainer for net\n","trainer = create_trainer(net)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087415,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"92O6ZKUd07C3"},"outputs":[],"source":["train_dataloader: gluon.data.DataLoader = data.get_dataloader(train=True)\n","val_dataloader: gluon.data.DataLoader = data.get_dataloader(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087414,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"MofpbTYp39gB"},"outputs":[],"source":["together_all_allx_seqs = np.concatenate([together_train_allx_seqs,together_test_allx_seqs],\n","                                   axis=0)\n","print(together_all_allx_seqs.shape)\n","together_all_y_seqs = np.concatenate([together_train_y_seqs,together_test_y_seqs],\n","                                   axis=0)\n","print(together_all_y_seqs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087412,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"t7Wfk22I35wR"},"outputs":[],"source":["# net1's prediction!\n","# note, batch is entire dataset's together x sequences\n","_batch = mx.nd.array(together_all_allx_seqs, ctx=ctx, dtype='float32')\n","_label = mx.nd.array(together_all_y_seqs, ctx=ctx, dtype='float32')\n","# _batch = _batch.astype('float32')\n","# _label = _label.astype('float32')\n","_batch = _batch.as_in_context(ctx)\n","_label = _label.as_in_context(ctx)\n","npx.set_np()\n","pred1 = net(_batch.as_np_ndarray())  # as_np_ndarray() to satiate an error\n","print('loss:', loss_fn(pred1, _label.as_np_ndarray()).mean())\n","print('loss:', loss_fn(pred1.flatten(), _label.as_np_ndarray().flatten()).mean())\n","print('L2Loss (MSE):', gluon.loss.L2Loss()(pred1, _label.as_np_ndarray()).mean())\n","print('L1Loss (MAE):', gluon.loss.L1Loss()(pred1, _label.as_np_ndarray()).mean())\n","print(pred1.shape)\n","print(pred1[0:3])\n","print(_label[0:3])\n","print()\n","pred1 = pred1.asnumpy()\n","real1 = _label.asnumpy()\n","pred1 = pred1.flatten()\n","real1 = real1.flatten()\n","print(pred1[0:3])\n","print(real1[0:3])\n","print()\n","\n","r2_1 = calc_r2(real1,pred1)\n","print(r2_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087411,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"917iUoP1CY-s"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","ax = plt.gca()\n","\n","_x = _unstdize(pred1)\n","_y = _unstdize(real1)\n","\n","ax.scatter(x=_x, y=_y, s=4, alpha=0.4)\n","ax.axline((0,0), slope=1, color='gray', linestyle='solid', linewidth=1)\n","\n","# limmax = max(_x.max(), _y.max())\n","# ax.set_ylim(0,limmax)\n","# ax.set_xlim(0,limmax)\n","ax.axis('equal')\n","ax.axis('square')\n","ax.set_ylabel('Actual $\\\\rm PM_{2.5}$')\n","ax.set_xlabel('Predicted $\\\\rm PM_{2.5}$')\n","ax.set_title('(a) NEP-LSTM v. 1 k=3 Actual vs Predicted')\n","\n","# line of best fit\n","_lob = np.poly1d(np.polyfit(_x, _y, deg=1))\n","print(_lob)\n","ax.axline((0,_lob[0]), slope=_lob[1], color='orange', linestyle='dashed', linewidth=1.5)"]},{"cell_type":"markdown","metadata":{"id":"CEi_FPEI0NTk"},"source":["## net2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087408,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Sg1Lqfx70w2X"},"outputs":[],"source":["data2 = ComplexCityDataSet(separate_train_targetx_seqs, separate_train_neighborx_seqs, separate_train_y_seqs,\n","                            separate_test_targetx_seqs, separate_test_neighborx_seqs, separate_test_y_seqs)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087406,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"TI9Ut7RI0oan"},"outputs":[],"source":["net2 = create_net_2(hidden_size=32, num_layers=3,\n","                    _ngbr_inner_dense_sizes=[32], _ngbr_out_dense_units=9)\n","print('net2:');  print(net2)\n","# reload from saved params\n","net2.load_parameters('params2/test_net_2_latest.params', ctx=ctx)\n","print('net2:');  print(net2)\n","\n","## trainer for net\n","trainer = create_trainer(net2)\n","loss_fn = create_loss_fn()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087405,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"E_1faJYZ08WZ"},"outputs":[],"source":["train_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=True)\n","val_dataloader2: gluon.data.DataLoader = data2.get_dataloader(train=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087404,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"PIjj_pFdMdEP"},"outputs":[],"source":["separate_all_targetx_seqs = np.concatenate([separate_train_targetx_seqs,separate_test_targetx_seqs],\n","                                   axis=0)\n","print(separate_all_targetx_seqs.shape)\n","separate_all_neighborx_seqs = np.concatenate([separate_train_neighborx_seqs,separate_test_neighborx_seqs],\n","                                   axis=0)\n","print(separate_all_neighborx_seqs.shape)\n","separate_all_y_seqs = np.concatenate([separate_train_y_seqs,separate_test_y_seqs],\n","                                   axis=0)\n","print(separate_all_y_seqs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087402,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"BPeJvC3dMd9Y"},"outputs":[],"source":["# net2's prediction!\n","# note, batch is entire dataset's together x sequences\n","_batchtgt = mx.nd.array(separate_all_targetx_seqs, ctx=ctx, dtype='float32')\n","_batchngbr = mx.nd.array(separate_all_neighborx_seqs, ctx=ctx, dtype='float32')\n","_label = mx.nd.array(separate_all_y_seqs, ctx=ctx, dtype='float32')\n","# _batch = _batch.astype('float32')\n","# _label = _label.astype('float32')\n","_batchtgt = _batchtgt.as_in_context(ctx)\n","_batchngbr = _batchngbr.as_in_context(ctx)\n","_label = _label.as_in_context(ctx)\n","npx.set_np()\n","pred2 = net2(_batchtgt.as_np_ndarray(), _batchngbr.as_np_ndarray())  # as_np_ndarray() to satiate an error\n","print('loss:', loss_fn(pred2, _label.as_np_ndarray()).mean())\n","print('loss:', loss_fn(pred2.flatten(), _label.as_np_ndarray().flatten()).mean())\n","print('L2Loss (MSE):', gluon.loss.L2Loss()(pred2, _label.as_np_ndarray()).mean())\n","print('L1Loss (MAE):', gluon.loss.L1Loss()(pred2, _label.as_np_ndarray()).mean())\n","print(pred2.shape)\n","print(pred2[0:3])\n","print(_label[0:3])\n","print()\n","pred2 = pred2.asnumpy()\n","real2 = _label.asnumpy()\n","pred2 = pred2.flatten()\n","real2 = real2.flatten()\n","print(pred2[0:3])\n","print(real2[0:3])\n","print()\n","\n","r2_2 = calc_r2(real2,pred2)\n","print(r2_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087401,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"29bpt2M1SUyZ"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","ax = plt.gca()\n","\n","_x = _unstdize(pred2)\n","_y = _unstdize(real2)\n","\n","ax.scatter(x=_x, y=_y, s=4, alpha=0.4, color='tab:purple')\n","ax.axline((0,0), slope=1, color='gray', linestyle='solid', linewidth=1)\n","\n","# limmax = max(_x.max(), _y.max())\n","# ax.set_ylim(0,limmax)\n","# ax.set_xlim(0,limmax)\n","ax.axis('equal')\n","ax.axis('square')\n","ax.set_ylabel('Actual $\\\\rm PM_{2.5}$')\n","ax.set_xlabel('Predicted $\\\\rm PM_{2.5}$')\n","ax.set_title('(a) Model 2 n=3 Actual vs Predicted')\n","\n","# line of best fit\n","_lob = np.poly1d(np.polyfit(_x, _y, deg=1))\n","print(_lob)\n","ax.axline((0,_lob[0]), slope=_lob[1], color='orange', linestyle='dashed', linewidth=1.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087399,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"yI0ERMvNUS-7"},"outputs":[],"source":["print(r2_1)\n","print(r2_2)"]},{"cell_type":"markdown","metadata":{"id":"YqPHLuzgS9T2"},"source":["### Plot both"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087397,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"cNBptaDdSVfv"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","\n","_x1 = _unstdize(pred1)\n","_y1 = _unstdize(real1)\n","\n","ax1.scatter(x=_x1, y=_y1, s=4, alpha=0.4, color='tab:blue')\n","ax1.axline((0,0), slope=1, color='gray', linestyle='solid', linewidth=1)\n","\n","# limmax = max(_x1.max(), _y1.max())\n","# ax1.set_ylim(0,limmax)\n","# ax1.set_xlim(0,limmax)\n","ax1.axis('equal')\n","ax1.axis('square')\n","ax1.set_ylabel('Actual $\\\\rm PM_{2.5}$ $(\\\\mu g/m^3)$')\n","ax1.set_xlabel('Predicted $\\\\rm PM_{2.5}$ $(\\\\mu g/m^3)$')\n","ax1.set_title('(a) NEP-LSTM variant 1')\n","\n","# line of best fit\n","_lob1 = np.poly1d(np.polyfit(_x1, _y1, deg=1))\n","print(_lob1)\n","ax1.axline((0,_lob1[0]), slope=_lob1[1], color='orange', linestyle='dashed', linewidth=1.5)\n","\n","\n","\n","_x2 = _unstdize(pred2)\n","_y2 = _unstdize(real2)\n","\n","ax2.scatter(x=_x2, y=_y2, s=4, alpha=0.4, color='tab:purple')\n","ax2.axline((0,0), slope=1, color='gray', linestyle='solid', linewidth=1)\n","\n","# limmax = max(_x2.max(), _y2.max())\n","# ax2.set_ylim(0,limmax)\n","# ax2.set_xlim(0,limmax)\n","ax2.axis('equal')\n","ax2.axis('square')\n","ax2.set_ylabel('Actual $\\\\rm PM_{2.5}$ $(\\\\mu g/m^3)$')\n","ax2.set_xlabel('Predicted $\\\\rm PM_{2.5}$ $(\\\\mu g/m^3)$')\n","ax2.set_title('(b) NEP-LSTM variant 2')\n","\n","# line of best fit\n","_lob2 = np.poly1d(np.polyfit(_x2, _y2, deg=1))\n","print(_lob2)\n","ax2.axline((0,_lob2[0]), slope=_lob2[1], color='orange', linestyle='dashed', linewidth=1.5)\n","\n","\n","\n","ax1.text(0.05, 0.95,\n","         '$\\\\rm MSE = 0.200$' +'\\n'+ '$\\\\rm R^{2} = '+f'{r2_1:.3f}$',\n","         horizontalalignment='left', verticalalignment='top', transform=ax1.transAxes)\n","ax2.text(0.05, 0.95,\n","         '$\\\\rm MSE = 0.231$' +'\\n'+ '$\\\\rm R^{2} = '+f'{r2_2:.3f}$',\n","         horizontalalignment='left', verticalalignment='top', transform=ax2.transAxes)\n"]},{"cell_type":"markdown","metadata":{"id":"u7K4xSR67w68"},"source":["# Feature importance by permutation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087395,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"S6C5GLKkk1e-"},"outputs":[],"source":["together_all_allx_seqs = np.concatenate([together_train_allx_seqs,together_test_allx_seqs],\n","                                   axis=0)\n","print(together_all_allx_seqs.shape)\n","together_all_y_seqs = np.concatenate([together_train_y_seqs,together_test_y_seqs],\n","                                   axis=0)\n","print(together_all_y_seqs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087394,"status":"aborted","timestamp":1701572401175,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"K3NR1wdGlAs_"},"outputs":[],"source":["separate_all_targetx_seqs = np.concatenate([separate_train_targetx_seqs,separate_test_targetx_seqs],\n","                                   axis=0)\n","print(separate_all_targetx_seqs.shape)\n","separate_all_neighborx_seqs = np.concatenate([separate_train_neighborx_seqs,separate_test_neighborx_seqs],\n","                                   axis=0)\n","print(separate_all_neighborx_seqs.shape)\n","separate_all_y_seqs = np.concatenate([separate_train_y_seqs,separate_test_y_seqs],\n","                                   axis=0)\n","print(separate_all_y_seqs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087392,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"p-7GF8vXlC99"},"outputs":[],"source":["# ensure that copying the array will not modify the original\n","print(separate_all_targetx_seqs[1])\n","_xcopy = separate_all_targetx_seqs.copy()\n","_xcopy[:,:,2] = 0\n","print(print(_xcopy[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087390,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"YGtg0ez68Epz"},"outputs":[],"source":["def feature_importance(net, inputs, labely, count=10):\n","  assert isinstance(inputs, (tuple, list)), 'provide a list of one or more inputs to the net'\n","  for inp in inputs:\n","    print(inp.shape)\n","  total_features = sum([inp.shape[2] for inp in inputs])\n","  print('total features =', total_features)\n","  print('labely.shape:', labely.shape)\n","  rng = np.random.default_rng()\n","\n","  MSEs = {fi: [] for fi in range(total_features)}\n","  MAEs = {fi: [] for fi in range(total_features)}\n","  R2s =  {fi: [] for fi in range(total_features)}\n","\n","  # provide the defaults\n","\n","  pred = net(*[mx.nd.array(inp,\n","                  ctx=ctx, dtype='float32') \\\n","                  .as_np_ndarray()\n","              for inp in inputs])\n","  label_to_use = mx.nd.array(labely,\n","                    ctx=ctx, dtype='float32') \\\n","                    .as_np_ndarray()\n","  mse = gluon.loss.L2Loss()(pred, label_to_use).mean()\n","  mae = gluon.loss.L1Loss()(pred, label_to_use).mean()\n","  print('L2Loss (MSE):', mse)\n","  print('L1Loss (MAE):', mae)\n","  r2 = calc_r2(label_to_use.asnumpy().flatten(),\n","                pred.asnumpy().flatten())\n","  orig_MSE = float(mse)\n","  orig_MAE = float(mae)\n","  orig_R2 = float(r2)\n","\n","  # go and permute\n","\n","  for ci in range(count):\n","    print()\n","    print('COUNT:', ci)\n","\n","    for fi in range(total_features):\n","        print('fi:', fi)\n","\n","        # permute an input and prepare the inputs\n","        inputs_to_use = []\n","        _featct = 0\n","\n","        for inp in inputs:\n","          inpprepped = None\n","          # permute feature at index fi in the input\n","          _feats = inp.shape[2]\n","          print('_feats =', _feats)\n","          _potential_fi = fi - _featct\n","          if 0 <= _potential_fi < _feats:\n","            print('permuting, _potential_fi =',_potential_fi)\n","            inpprepped = inp.copy()\n","            # permute the slice at feature index _potential_fi\n","            inpprepped[:,:,_potential_fi] = rng.permuted(\n","                inpprepped[:,:,_potential_fi],\n","                axis=0\n","            )\n","          else:\n","            inpprepped = inp\n","          # prepare input\n","          inpprepped = mx.nd.array(inpprepped,\n","                                   ctx=ctx, dtype='float32') \\\n","                                   .as_np_ndarray()  # satiates an error\n","          # go to next input\n","          inputs_to_use.append(inpprepped)\n","          _featct += _feats\n","        \n","        # now, inputs_to_use is populated\n","        assert len(inputs_to_use) == len(inputs)\n","\n","        label_to_use = mx.nd.array(labely,\n","                                   ctx=ctx, dtype='float32') \\\n","                                   .as_np_ndarray()  # satiates an error\n","        # predict\n","        pred = net(*inputs_to_use)\n","        mse = gluon.loss.L2Loss()(pred, label_to_use).mean()\n","        mae = gluon.loss.L1Loss()(pred, label_to_use).mean()\n","        print('L2Loss (MSE):', mse)\n","        print('L1Loss (MAE):', mae)\n","        r2 = calc_r2(label_to_use.asnumpy().flatten(),\n","                     pred.asnumpy().flatten())\n","        MSEs[fi].append(float(mse))\n","        MAEs[fi].append(float(mae))\n","        R2s[fi].append(float(r2))\n","\n","    # end of count\n","\n","  # done with all counts\n","  return orig_MSE, orig_MAE, orig_R2, MSEs, MAEs, R2s"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087388,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"pFY92-m3khG-"},"outputs":[],"source":["orig_MSE, orig_MAE, orig_R2, MSEs, MAEs, R2s = \\\n","feature_importance(net, inputs=(together_all_allx_seqs,),\n","                   labely=together_all_y_seqs, count=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087387,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"XrcCATnwzOjl"},"outputs":[],"source":["print(orig_MSE, '--', MSEs)\n","print(orig_MAE, '--', MAEs)\n","print(orig_R2, '--', R2s)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087385,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"07DKejoR76eo"},"outputs":[],"source":["featnames = ['$PM_{10}$', 'CO', '$SO_2$', '$O_3$', '$NO_2$', 'WS'] \\\n","            + ['$PM_{2.5}$ n #0', '$PM_{10}$ n #0', 'CO n #0', '$SO_2$ n #0', '$O_3$ n #0',\n","               '$NO_2$ n #0', 'WS n #0', 'WD.max n #0', 'distance n #0'] \\\n","            + ['$PM_{2.5}$ n #1', '$PM_{10}$ n #1', 'CO n #1', '$SO_2$ n #1', '$O_3$ n #1',\n","               '$NO_2$ n #1', 'WS n #1', 'WD.max n #1', 'distance n #1'] \\\n","            + ['$PM_{2.5}$ n #2', '$PM_{10}$ n #2', 'CO n #2', '$SO_2$ n #2', '$O_3$ n #2',\n","               '$NO_2$ n #2', 'WS n #2', 'WD.max n #2', 'distance n #2']"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087383,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"TaQmadG7zU48"},"outputs":[],"source":["for fi in MSEs.keys():\n","    plt.plot([*zip(np.array(MSEs[fi])-orig_MSE,\n","                   np.array(MAEs[fi])-orig_MAE,\n","                   np.array(R2s[fi])-orig_R2\n","                   )],\n","             label=f'fi {fi}')\n","plt.show()\n","\n","fig = plt.figure(figsize=(8,8))\n","ax = fig.gca()\n","ax.set_title('MSEs importances:')\n","keys = list(MSEs.keys())\n","ax.barh(range(len(MSEs)),\n","        [\n","            (np.array(MSEs[fi])-orig_MSE).mean()\n","            for fi in keys\n","        ],\n","        xerr=[\n","            (np.array(MSEs[fi])-orig_MSE).std()\n","            for fi in keys\n","        ]\n","         )\n","ax.set_yticks(range(len(MSEs)))\n","_=ax.set_yticklabels([featnames[key] for key in keys])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087382,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"Ve6FMdgCzqTD"},"outputs":[],"source":["orig_MSE, orig_MAE, orig_R2, MSEs, MAEs, R2s = \\\n","feature_importance(net2, inputs=(separate_all_targetx_seqs, separate_all_neighborx_seqs),\n","                   labely=together_all_y_seqs, count=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087381,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"6ZumtBusWVTC"},"outputs":[],"source":["print(orig_MSE, '--', MSEs)\n","print(orig_MAE, '--', MAEs)\n","print(orig_R2, '--', R2s)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087380,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"-_VxYgLsYgmO"},"outputs":[],"source":["fig = plt.figure(figsize=(8,8))\n","ax = fig.gca()\n","ax.set_title('MSEs importances:')\n","keys = list(MSEs.keys())\n","ax.barh(range(len(MSEs)),\n","        [\n","            (np.array(MSEs[fi])-orig_MSE).mean()\n","            for fi in keys\n","        ],\n","        xerr=[\n","            (np.array(MSEs[fi])-orig_MSE).std()\n","            for fi in keys\n","        ]\n","         )\n","ax.set_yticks(range(len(MSEs)))\n","_=ax.set_yticklabels([featnames[key] for key in keys])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1087378,"status":"aborted","timestamp":1701572401176,"user":{"displayName":"ZH Xia","userId":"08741985132073733515"},"user_tz":360},"id":"K51gTCX5YhG3"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
